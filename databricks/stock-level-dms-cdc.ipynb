{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ca4b769-f7c3-47ed-b813-1dc1555c4707",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Incrementally load parquet files from s3 (DMS generated with CDC) with Databricks' AutoLoader and merge to delta table in bronze layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "354fa437-2fc8-4d04-be0b-96547bfcff17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd61ca70-63e0-474b-9c6e-c115e19e0e76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- sample table that keep track of stock levels for all items from operational source which is RDBMS in this case\n",
    "-- created delta table with unity catalog enabled with 3 level namespace.\n",
    "\n",
    "create or replace table demo_catalog.demo_schema.stock_levels(\n",
    "  id bigint,\n",
    "  item_id bigint,\n",
    "  stock_level int,\n",
    "  mca_cost decimal(10, 5),\n",
    "  discontinued_date timestamp,\n",
    "  discontinued_flag int,\n",
    "  is_new int,\n",
    "  unit_cost decimal(10,5),\n",
    "  created_at timestamp,\n",
    "  updated_at timestamp,\n",
    "  deleted_at timestamp\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce2da3ab-a25b-4eaf-b9c1-4f3f8a348c8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#schema generated from DMS\n",
    "\n",
    "tschema = StructType([\n",
    "    StructField(\"Op\", StringType()),\n",
    "    StructField(\"operation_timestamp\", TimestampType()),\n",
    "    StructField(\"id\", LongType()),\n",
    "    StructField(\"item_id\", LongType()),\n",
    "    StructField(\"stock_level\", IntegerType()),\n",
    "    StructField('mca_cost',DecimalType(10,5)),\n",
    "    StructField('discontinued_date', TimestampType()),\n",
    "    StructField('discontinued_flag', IntegerType()),\n",
    "    StructField(\"deleted_at\", TimestampType()),\n",
    "    StructField(\"updated_at\", TimestampType()),\n",
    "    StructField(\"created_at\", TimestampType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85e19d5a-9334-4d70-a8b4-f400f447a1b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#read parquet files from S3 as streaming dataframe to use AutoLoader\n",
    "\n",
    "df = (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"parquet\")\n",
    "    .option(\"cloudFiles.schemaLocation\", \"s3://path\")\n",
    "    .load(\"s3://path\", schema=tschema)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd6031bf-c658-43ac-876b-b72f96d35734",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>item_id</th><th>stock_level</th><th>mca_cost</th><th>discontinued_date</th><th>discontinued_flag</th><th>is_new</th><th>unit_cost</th><th>created_at</th><th>updated_at</th><th>deleted_at</th></tr></thead><tbody><tr><td>1</td><td>1</td><td>100</td><td>null</td><td>null</td><td>0</td><td>1</td><td>15.00000</td><td>2023-11-16T10:17:09Z</td><td>2023-11-16T10:17:09Z</td><td>null</td></tr><tr><td>2</td><td>12</td><td>100</td><td>null</td><td>null</td><td>0</td><td>1</td><td>0.15888</td><td>2023-11-16T10:39:00Z</td><td>2023-11-16T10:39:00Z</td><td>null</td></tr><tr><td>3</td><td>14</td><td>100</td><td>null</td><td>null</td><td>0</td><td>1</td><td>4.15888</td><td>2023-11-16T10:40:24Z</td><td>2023-11-16T10:40:24Z</td><td>null</td></tr><tr><td>4</td><td>3</td><td>25</td><td>null</td><td>null</td><td>0</td><td>1</td><td>34.00000</td><td>2023-11-16T10:51:46Z</td><td>2023-11-16T10:51:46Z</td><td>null</td></tr><tr><td>5</td><td>34</td><td>25</td><td>null</td><td>null</td><td>0</td><td>1</td><td>34.00000</td><td>2023-11-16T10:51:53Z</td><td>2023-11-16T10:51:53Z</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         1,
         100,
         null,
         null,
         0,
         1,
         "15.00000",
         "2023-11-16T10:17:09Z",
         "2023-11-16T10:17:09Z",
         null
        ],
        [
         2,
         12,
         100,
         null,
         null,
         0,
         1,
         "0.15888",
         "2023-11-16T10:39:00Z",
         "2023-11-16T10:39:00Z",
         null
        ],
        [
         3,
         14,
         100,
         null,
         null,
         0,
         1,
         "4.15888",
         "2023-11-16T10:40:24Z",
         "2023-11-16T10:40:24Z",
         null
        ],
        [
         4,
         3,
         25,
         null,
         null,
         0,
         1,
         "34.00000",
         "2023-11-16T10:51:46Z",
         "2023-11-16T10:51:46Z",
         null
        ],
        [
         5,
         34,
         25,
         null,
         null,
         0,
         1,
         "34.00000",
         "2023-11-16T10:51:53Z",
         "2023-11-16T10:51:53Z",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"scale\":0}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{\"scale\":0}",
         "name": "item_id",
         "type": "\"long\""
        },
        {
         "metadata": "{\"scale\":0}",
         "name": "stock_level",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"scale\":5}",
         "name": "mca_cost",
         "type": "\"decimal(10,5)\""
        },
        {
         "metadata": "{\"scale\":0}",
         "name": "discontinued_date",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{\"scale\":0}",
         "name": "discontinued_flag",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"scale\":0}",
         "name": "is_new",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"scale\":5}",
         "name": "unit_cost",
         "type": "\"decimal(10,5)\""
        },
        {
         "metadata": "{\"scale\":0}",
         "name": "created_at",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{\"scale\":0}",
         "name": "updated_at",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{\"scale\":0}",
         "name": "deleted_at",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3a6a068-980c-4409-8755-90f86ac0f686",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.limit(20).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "515abc1b-5a30-4c30-b128-0ab196e85e37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Option 1: this is when you need the most recent data, same data with source table. <br>\n",
    "Then you can upsert the incoming rows from DMS into delta table as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "433e948e-f144-4e7d-b352-1dad25058842",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#get reference of delta table by name\n",
    "deltaTable = DeltaTable.forName(spark, \"demo_catalog.demo_schema.stock_levels\")\n",
    "\n",
    "#function to apply in foreachBatch() to write data into any storage\n",
    "def upsertToUC(microBatchOutputDF, batchId):\n",
    "\n",
    "    #get list of id column to delete in target table\n",
    "    #need to get list before drop duplicate operation\n",
    "    deleted_id_list = microBatchOutputDF.filter(microBatchOutputDF[\"Op\"] == \"D\").select(\"id\").toPandas()[\"id\"].tolist()\n",
    "    \n",
    "    #sorted and drop duplicates and get the most recent row only \n",
    "    #the merge will get error when trying to merge two rows from source to one row at target\n",
    "    microBatchOutputDF = microBatchOutputDF.sort(microBatchOutputDF[\"operation_timestamp\"].desc())\n",
    "    microBatchOutputDF = microBatchOutputDF.dropDuplicates([\"id\"])\n",
    "\n",
    "    microBatchOutputDF = microBatchOutputDF.drop(col(\"Op\"), col(\"operation_timestamp\"))\n",
    "\n",
    "    microBatchOutputDF = microBatchOutputDF.sort(microBatchOutputDF[\"id\"])\n",
    "\n",
    "    (\n",
    "\n",
    "        deltaTable.alias(\"t\")\\\n",
    "        .merge(microBatchOutputDF.alias(\"s\"), \"s.id = t.id\")\\\n",
    "        .whenMatchedUpdateAll()\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "    #delete after merging\n",
    "    deltaTable.delete(col(\"id\").isin(deleted_id_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e47a38a5-0845-42a2-a52f-90f4db4c98e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#write to bronze table incremenatlly \n",
    "#databricks will store metadata in the checkpoint location to track data changes and write the new data only.\n",
    "(\n",
    "    df.writeStream.option(\"checkpointLocation\", \"dbfs_path\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(availableNow=True)\n",
    "    .foreachBatch(upsertToUC)\n",
    "    .start()\n",
    "    .awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c1ce1f3-8dfa-407a-bfc3-9023f9f8f40e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from demo_catalog.demo_schema.stock_levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23f9a310-4e92-488f-a25e-c79b074b690b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Option 2: this is when you want all changes record in the delta table. <br>\n",
    "It is pretty straightforward, you just append all the rows generated from DMS to the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f1bf6c3-a3ac-4f54-a2b7-b760b953c1bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df.writeStream.option(\"checkpointLocation\", \"dbfs_path\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .mode(\"append\")\n",
    "    .trigger(availableNow=True)\n",
    "    .start()\n",
    "    .awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "572d8c2d-9049-4587-9e3e-f0b62653aa44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from demo_catalog.demo_schema.stock_levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a922c6da-fc34-4fcc-9009-e4a0e8bfef2d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "After you got all the changes for each row in the table, now you can create SCD tables in next step. <br>\n",
    "SCD table design will depend on your analytics requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efce068c-8aa0-4010-b571-8acd0359ec18",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from demo_catalog.demo_schema.stock_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5e7d959-4763-479f-baee-e7f87881b470",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a535efe-9d3d-45b4-8285-56fae357d244",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87f3d5f1-8fc6-4b9f-a38e-f56b9bffdb2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "All these code may or may not work depending on databricks cluster and spark configs you use."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1012917882111787,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "stock-level-dms-cdc",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
