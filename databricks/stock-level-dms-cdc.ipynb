{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ca4b769-f7c3-47ed-b813-1dc1555c4707",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Incrementally load parquet files from s3 (DMS generated with CDC) with Databricks' AutoLoader and merge to delta table in bronze layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "354fa437-2fc8-4d04-be0b-96547bfcff17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd61ca70-63e0-474b-9c6e-c115e19e0e76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- sample table that keep track of stock levels for all items from operational source which is RDBMS in this case\n",
    "-- created delta table with unity catalog enabled with 3 level namespace.\n",
    "\n",
    "create or replace table mycatalog.myschema.stock_levels(\n",
    "  id bigint,\n",
    "  item_id bigint,\n",
    "  stock_level int,\n",
    "  mca_cost decimal(10, 5),\n",
    "  discontinued_date timestamp,\n",
    "  discontinued_flag int,\n",
    "  is_new int,\n",
    "  unit_cost decimal(10,5),\n",
    "  created_at timestamp,\n",
    "  updated_at timestamp,\n",
    "  deleted_at timestamp\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce2da3ab-a25b-4eaf-b9c1-4f3f8a348c8d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#schema generated from DMS\n",
    "\n",
    "tschema = StructType([\n",
    "    StructField(\"Op\", StringType()),\n",
    "    StructField(\"operation_timestamp\", TimestampType()),\n",
    "    StructField(\"id\", LongType()),\n",
    "    StructField(\"item_id\", LongType()),\n",
    "    StructField(\"stock_level\", IntegerType()),\n",
    "    StructField('mca_cost',DecimalType(10,5)),\n",
    "    StructField('discontinued_date', TimestampType()),\n",
    "    StructField('discontinued_flag', IntegerType()),\n",
    "    StructField(\"deleted_at\", TimestampType()),\n",
    "    StructField(\"updated_at\", TimestampType()),\n",
    "    StructField(\"created_at\", TimestampType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85e19d5a-9334-4d70-a8b4-f400f447a1b4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#read parquet files from S3 as streaming dataframe to use AutoLoader\n",
    "\n",
    "df = (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"parquet\")\n",
    "    .option(\"cloudFiles.schemaLocation\", \"s3://path\")\n",
    "    .load(\"s3://path\", schema=tschema)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "515abc1b-5a30-4c30-b128-0ab196e85e37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Option 1: when you need the most recent data, same data with source table. <br>\n",
    "Then you can upsert the incoming rows from DMS into delta table as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "433e948e-f144-4e7d-b352-1dad25058842",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#get reference of delta table by name\n",
    "deltaTable = DeltaTable.forName(spark, \"mycatalog.myschema.stock_levels\")\n",
    "\n",
    "#function to apply in foreachBatch() to write data into any storage\n",
    "def upsertToUC(microBatchOutputDF, batchId):\n",
    "\n",
    "    #get list of id column to delete in target table\n",
    "    #need to get list before drop duplicate operation\n",
    "    deleted_id_list = microBatchOutputDF.filter(microBatchOutputDF[\"Op\"] == \"D\").select(\"id\").toPandas()[\"id\"].tolist()\n",
    "    \n",
    "    #sorted and drop duplicates and get the most recent row only \n",
    "    #the merge will get error when trying to merge two rows from source to one row at target\n",
    "    microBatchOutputDF = microBatchOutputDF.sort(microBatchOutputDF[\"operation_timestamp\"].desc())\n",
    "    microBatchOutputDF = microBatchOutputDF.dropDuplicates([\"id\"])\n",
    "\n",
    "    microBatchOutputDF = microBatchOutputDF.drop(col(\"Op\"), col(\"operation_timestamp\"))\n",
    "\n",
    "    microBatchOutputDF = microBatchOutputDF.sort(microBatchOutputDF[\"id\"])\n",
    "\n",
    "    (\n",
    "\n",
    "        deltaTable.alias(\"t\")\\\n",
    "        .merge(microBatchOutputDF.alias(\"s\"), \"s.id = t.id\")\\\n",
    "        .whenMatchedUpdateAll()\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()\n",
    "    )\n",
    "\n",
    "    #delete after merging\n",
    "    deltaTable.delete(col(\"id\").isin(deleted_id_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e47a38a5-0845-42a2-a52f-90f4db4c98e0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#write to bronze table incremenatlly \n",
    "#databricks will store metadata in the checkpoint location to track data changes and write the new data only.\n",
    "(\n",
    "    df.writeStream.option(\"checkpointLocation\", \"dbfs_path\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(availableNow=True)\n",
    "    .foreachBatch(upsertToUC)\n",
    "    .start()\n",
    "    .awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23f9a310-4e92-488f-a25e-c79b074b690b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Option 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87f3d5f1-8fc6-4b9f-a38e-f56b9bffdb2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#coming soon"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1012917882111787,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "stock-level-dms-cdc",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
